---
title: "CAI's experiments"
output: html_notebook
---

### Library Environment
```{r}
suppressMessages(library(tidyverse))
suppressMessages(library(stringr))
suppressMessages(library(ISLR))
suppressMessages(library(caret))
suppressMessages(library(doMC))
suppressMessages(library(plotly))
suppressMessages(library(stringr))
registerDoMC(cores=4)
```

### Load and processing data ctu13 cleaned
```{r}
myData_cleaned <- read.csv('/home/jguerra/datasets/ctu13.labeled.cleaned', stringsAsFactors = F, sep = '|')
myData_cleaned.bkp = myData_cleaned
myData_cleaned

#Periodicity
myData_cleaned = myData_cleaned %>% mutate(strong_p = str_count(State,'[a-i]'))
myData_cleaned = myData_cleaned %>% mutate(weak_p = str_count(State,'[A-I]'))
myData_cleaned = myData_cleaned %>% mutate(weak_np = str_count(State,'[r-z]'))
myData_cleaned = myData_cleaned %>% mutate(strong_np = str_count(State,'[R-Z]'))
#Duration
myData_cleaned = myData_cleaned %>% mutate(duration_s = str_count(State,'(a|A|r|R|1|d|D|u|U|4|g|G|x|X|7)'))
myData_cleaned = myData_cleaned %>% mutate(duration_m = str_count(State,'(b|B|s|S|2|e|E|v|V|5|h|H|y|Y|8)'))
myData_cleaned = myData_cleaned %>% mutate(duration_l = str_count(State,'(c|C|t|T|3|f|F|w|W|6|i|I|z|Z|9)'))
#Size
myData_cleaned = myData_cleaned %>% mutate(size_s = str_count(State,'[a-c]') + str_count(State,'[A-C]') + str_count(State,'[r-t]') + str_count(State,'[R-T]') + str_count(State,'[1-3]'))
myData_cleaned = myData_cleaned %>% mutate(size_m = str_count(State,'[d-f]') + str_count(State,'[D-F]') + str_count(State,'[u-w]') + str_count(State,'[U-W]') + str_count(State,'[4-6]'))
myData_cleaned = myData_cleaned %>% mutate(size_l = str_count(State,'[g-i]') + str_count(State,'[G-I]') + str_count(State,'[x-z]') + str_count(State,'[X-Z]') + str_count(State,'[7-9]'))

#Periodicity %
myData_cleaned <- myData_cleaned %>% mutate(strong_p = (strong_p / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(weak_p = (weak_p / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(strong_np = (strong_np / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(weak_np = (weak_np / modelsize))
#Duration %
myData_cleaned <- myData_cleaned %>% mutate(duration_s = (duration_s / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(duration_m = (duration_m / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(duration_l = (duration_l / modelsize))
#Size %
myData_cleaned <- myData_cleaned %>% mutate(size_s = (size_s / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(size_m = (size_m / modelsize))
myData_cleaned <- myData_cleaned %>% mutate(size_l = (size_l / modelsize))

#Making feature vectors
feature_vectors_cleaned = myData_cleaned[,c('strong_p','weak_p','weak_np','strong_np','duration_s','duration_m','duration_l','size_s','size_m','size_l','modelsize','label','class','port','proto')]
names(feature_vectors_cleaned) = c("sp","wp","wnp","snp","ds","dm","dl","ss","sm","sl","modelsize","class","subclass","port","proto")
feature_vectors_cleaned$class = factor(feature_vectors_cleaned$class)
feature_vectors_cleaned$subclass = factor(feature_vectors_cleaned$subclass)
feature_vectors_cleaned$proto = factor(feature_vectors_cleaned$proto)

feature_vectors_cleaned

```

### Create training set and testset
```{r}
set.seed(212)
trainIndex <- createDataPartition(feature_vectors_cleaned$subclass, p=0.70, list=FALSE)
data_training <- feature_vectors_cleaned[ trainIndex,]
data_testing <- feature_vectors_cleaned[-trainIndex,]

#data_train = data_train %>% filter(length>5)
train <- upSample(x = data_training,  y = data_training$subclass, yname="class")

training <- train[,-c(11,16)]
testing <- data_testing[,-c(11)]
training
testing

nrow(training)
nrow(feature_vectors_cleaned)

```

### Training configuration
```{r}
ctrl_fast <- trainControl(method="cv", 
                     repeats=2,
                     number=10, 
                     summaryFunction=twoClassSummary,
                     verboseIter=T,
                     classProbs=TRUE,
                     allowParallel = TRUE)  
```

### Experiment 1
## Creation of cluster and k parameters analysis
```{r}
library(factoextra)
library(cluster)
library(NbClust)
feature_vector_training = training[,-c(11,12,13,14)]
# K-means clustering
set.seed(321)
#km.res <- kmeans(feature_vector_training, 3, nstart = 25)
km.res <- kmeans(feature_vector_training, 7, nstart = 25)
# k-means group number of each observation
km.res$cluster

# Visualize k-means clusters
fviz_cluster(km.res, data = feature_vector_training, geom = "point",
             stand = FALSE, ellipse.type = "norm")
```
### Elbow analysis
```{r}
set.seed(321)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
data <- feature_vector_training
wss <- sapply(1:k.max, 
        function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k.max, wss,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
abline(v = 3, lty =2)
```
## Silhouette analysis
```{r}
set.seed(322)
k.max <- 10
data <- feature_vector_training
nrow(data)
sil <- rep(0, k.max)
# Compute the average silhouette width for 
# k = 2 to k = 15

for(i in 2:k.max){
  km.res <- kmeans(data, centers = i, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(data))
  sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

```
### Useful function
```{r}
cold_start_graphic <- function(training.sampled,testing,settings){
  library(doParallel)
  cl <- makeCluster(2)
  registerDoParallel(cl)
  size_training <- nrow(training.sampled)
  split_size_training = size_training / 200
  
  count_random <- foreach(i=1:split_size_training) %dopar% {
    200 * i
  }
  metric <- foreach(i=1:split_size_training) %dopar% {
    library(caret)
    #library(dplyr)
    count <- 200 * i
    aux_training_set <- training.sampled[c(1:count), ]#training[sample(size_training, count), ]
    clusters <- kmeans(aux_training_set[,-c(11,12,13,14)],3,nstart = 25)
    aux_training_set_cluster <- cbind(aux_training_set, cluster = clusters$cluster)
    result_vector <- numeric(nrow(testing))
    
    for (j in c(1:3)){
      cluster_data <- dplyr::filter(aux_training_set_cluster, cluster == j)
      new_rfFit <- train(subclass ~ sp+wp+wnp+snp+ds+dm+dl+ss+sm+sl,
                 data = cluster_data,
                 metric="ROC",
                 method = "rf",
                 trControl = settings)
      predsrfprobs <- predict(new_rfFit,testing,type='prob')
      
      for (k in c(1:length(result_vector))){
        if(predsrfprobs$botnet[k] > 0.5){
          result_vector[k] <- result_vector[k] + 1
        }
        else{
          result_vector[k] <- result_vector[k] - 1
        }
      }
    }
    a = ifelse(result_vector > 0,'botnet','normal')
    cm <- confusionMatrix(a,testing$subclass)
    metric <- cm$byClass['F1']#cm$overall[1]
    metric
  }
  output <- do.call(rbind, Map(data.frame, data_count=count_random, metric=metric))
  output
  gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
}

cold_start_data <- function(training.sampled,testing,settings){
  library(doParallel)
  cl <- makeCluster(2)
  registerDoParallel(cl)
  size_training <- nrow(training.sampled)
  split_size_training = size_training / 200
  
  count_random <- foreach(i=1:split_size_training) %dopar% {
    200 * i
  }
  metric <- foreach(i=1:split_size_training) %dopar% {
    library(caret)
    #library(dplyr)
    count <- 200 * i
    aux_training_set <- training.sampled[c(1:count), ]#training[sample(size_training, count), ]
    clusters <- kmeans(aux_training_set[,-c(11,12,13,14)],3,nstart = 25)
    aux_training_set_cluster <- cbind(aux_training_set, cluster = clusters$cluster)
    result_vector <- numeric(nrow(testing))
    
    for (j in c(1:3)){
      cluster_data <- dplyr::filter(aux_training_set_cluster, cluster == j)
      new_rfFit <- train(subclass ~ sp+wp+wnp+snp+ds+dm+dl+ss+sm+sl,
                 data = cluster_data,
                 metric="ROC",
                 method = "rf",
                 trControl = settings)
      predsrfprobs <- predict(new_rfFit,testing,type='prob')
      
      for (k in c(1:length(result_vector))){
        if(predsrfprobs$botnet[k] > 0.5){
          result_vector[k] <- result_vector[k] + 1
        }
        else{
          result_vector[k] <- result_vector[k] - 1
        }
      }
    }
    a = ifelse(result_vector > 0,'botnet','normal')
    cm <- confusionMatrix(a,testing$subclass)
    metric <- cm$byClass['F1']#cm$overall[1]
    metric
  }
  output <- do.call(rbind, Map(data.frame, data_count=count_random, metric=metric))
  output
}
```

### Data training partitions: cold start study
### Iteration #1
```{r}
set.seed(201)
size_training <- nrow(training)
training.sampled_1 <- training[sample(size_training, size_training), ]

output <- cold_start_data(training.sampled_1, testing, settings = ctrl_fast)
output
gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
first_training_sample <- training.sampled_1[1:200,]
ggplot(first_training_sample) + geom_bar(aes(subclass))
class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
### Iteration #2
```{r}
set.seed(202)
size_training <- nrow(training)
training.sampled_2 <- training[sample(size_training, size_training), ]

output <- cold_start_data(training.sampled_2, testing, settings = ctrl_fast)
output
gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
first_training_sample <- training.sampled_2[1:200,]
ggplot(first_training_sample) + geom_bar(aes(subclass))
class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Iteration #3
```{r}
set.seed(2013)
size_training <- nrow(training)
training.sampled_3 <- training[sample(size_training, size_training), ]

output <- cold_start_data(training.sampled_3, testing, settings = ctrl_fast)
output
gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
first_training_sample <- training.sampled_3[1:200,]
ggplot(first_training_sample) + geom_bar(aes(subclass))
class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Iteration #4
```{r}
set.seed(204)
size_training <- nrow(training)
training.sampled_4 <- training[sample(size_training, size_training), ]

output <- cold_start_data(training.sampled_4, testing, settings = ctrl_fast)
output
gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
first_training_sample <- training.sampled_4[1:200,]
ggplot(first_training_sample) + geom_bar(aes(subclass))
class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Iteration #5
```{r}
set.seed(205)
size_training <- nrow(training)
training.sampled_5 <- training[sample(size_training, size_training), ]

output <- cold_start_data(training.sampled_5, testing, settings = ctrl_fast)
output
gg <- ggplot(data = output)
  gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
    labs(title="Random Forest through data training size", 
         #subtitle="Drawn from Long Data format", 
         caption="Source: CTU-13", 
         y="F1 Score", 
         color=NULL)
first_training_sample <- training.sampled_5[1:200,]
ggplot(first_training_sample) + geom_bar(aes(subclass))
class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



### Studies Samples
```{r}
first_training_sample <- training.sampled[1:200,]
first_training_sample
ggplot(first_training_sample) + geom_bar(aes(subclass))

class_distribution <- first_training_sample %>% group_by(class) %>% summarise(n = n()) %>% arrange(desc(n))
ggplot(first_training_sample) + geom_bar(aes(class)) + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


### part 2
```{r}
set.seed(206)
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
size_training <- nrow(training)
split_size_training = size_training / 200
count_random <- foreach(i=1:split_size_training) %dopar% {
  200 * i
}
training.sampled <- training[sample(size_training, size_training), ]
metric <- foreach(i=1:split_size_training) %do% {
  #library(caret)
  count <- 200 * i
  aux_training_set <- training.sampled[c(1:count), ]#training[sample(size_training, count), ]
  clusters <- kmeans(aux_training_set[,-c(11,12,13,14)],3,nstart = 25)
  aux_training_set_cluster <- cbind(aux_training_set, cluster = clusters$cluster)
  result_vector <- numeric(nrow(testing))
  for (j in c(1:3)){
    cluster_data <- filter(aux_training_set_cluster, cluster == j)
    new_rfFit <- train(class ~ sp+wp+wnp+snp+ds+dm+dl+ss+sm+sl,
               data = cluster_data,
               metric="ROC",
               method = "rf",
               trControl = ctrl_fast)
    predsrfprobs <- predict(new_rfFit,testing,type='prob')
    for (k in c(1:length(result_vector))){
      if(predsrfprobs$Botnet[k] > 0.5){
        result_vector[k] <- result_vector[k] + 1
      }
      else{
        result_vector[k] <- result_vector[k] - 1
      }
    }
    
  }
  a = ifelse(result_vector > 0,'Botnet','Normal')
  cm <- confusionMatrix(a,testing$class)
  metric <- cm$byClass['F1']#cm$overall[1]
  metric
  
}

output <- do.call(rbind, Map(data.frame, data_count=count_random, metric=metric))
output
gg <- ggplot(data = output)
gg + geom_line(aes(x = data_count, y = metric),color = 'red') + 
  labs(title="Random Forest through data training size", 
       #subtitle="Drawn from Long Data format", 
       caption="Source: CTU-13", 
       y="Accuracy", 
       color=NULL)
cluster_data
```


### Test with only one
```{r}
set.seed(206)
size_training <- nrow(training)
training.sampled <- training[sample(size_training, size_training), ]

aux_training_set <- training.sampled[c(1:200), ]#training[sample(size_training, 200), ]
clusters <- kmeans(aux_training_set[,-c(11,12,13,14)],3,nstart = 25)
aux_training_set_cluster <- cbind(aux_training_set, cluster = clusters$cluster)
result_vector <- numeric(nrow(testing))
for (j in c(1:3)){
  cluster_data <- aux_training_set_cluster %>% filter(cluster == j)
  new_rfFit <- train(subclass ~ sp+wp+wnp+snp+ds+dm+dl+ss+sm+sl,
               data = cluster_data,
               metric="ROC",
               method = "rf",
               trControl = ctrl_fast)
  predsrfprobs <- predict(new_rfFit,testing,type='prob')
  for (k in c(1:length(result_vector))){
    if(predsrfprobs$botnet[k] > 0.5){
      result_vector[k] <- result_vector[k] + 1
    }
    else{
      result_vector[k] <- result_vector[k] - 1
    }
  }
}

a = ifelse(result_vector > 0,'botnet','normal')
cm <- confusionMatrix(a,testing$subclass)
metric <- cm$byClass['F1']
metric
cm$byClass['F1']

```

### Sample examples
```{r}
set.seed(556)
a = c(1,2,3,4,5,6,7,8,9)
r <- sample(9,3)
a[r]
r2 <- sample(9,3)
a[r2]
```

